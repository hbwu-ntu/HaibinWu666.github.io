---
permalink: /
title: ""
excerpt: "About me"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---


# About Me

Hi! I'm Haibin Wu. I am currently a fourth-year Ph.D. student at the college of Electrical Engineering and Computer Science (EECS) at National Taiwan University (NTU). I am a member of the Speech Processing Lab, working with Prof. [Hung-yi Lee](http://speech.ee.ntu.edu.tw/~tlkagk/) and Prof. [Lin-shan Lee](http://speech.ee.ntu.edu.tw/previous_version/lslNew.htm) in the area of machine learning and speech processing. My expertise lies in speech foundation models, prompt engineer, speech LLMs, speech enhancement, and speaker verification, audio event classification, multimodal learning and model compression. By the way. I was fortunate enough to be funded by a [Google PhD Fellowship](https://research.google/outreach/phd-fellowship/recipients/). I'm a main contributor for [S3PRL](https://github.com/s3prl/s3prl) v0.4.0 with 2000+ GitHub stars. I have a keen interest in photography, and you can find my portfolio on my [homepage](https://www.mipai.com.cn/frankwu).

I am now looking for full-time research scientist positions in the fields of speech processing and machine learning. I would greatly appreciate your assistance if you have any opportunities or recommendations in these areas.

<!-- 
[Publications](#publications) / [Teaching](#teaching) / [Honors](#honors) / [Side Projects](#projects) / [CV](files/cv.pdf)
-->
<!-- 
/ [Talks](#Talks) 
-->

# Selected Publications

- **EMO-SUPERB: An In-depth Look at Speech Emotion Recognition**<br/>
    <u>Haibin Wu</u>, Huang-Cheng Chou, Kai-Wei Chang, Lucas Goncalves, Jiawei Du, Jyh-Shing Roger Jang, Chi-Chun Lee, Hung-Yi Lee<br/>
    *Preprint*<br/>
    [ [pdf](https://arxiv.org/abs/2402.13018) | [Webpage](https://emosuperb.github.io/) | [Github](https://github.com/EMOsuperb/EMO-SUPERB-submission)]

- **Codec-SUPERB: An In-Depth Analysis of Sound Codec Models**<br/>
    <u>Haibin Wu</u>, Ho-Lam Chung, Yi-Cheng Lin, Yuan-Kuei Wu, Xuanjun Chen, Yu-Chi Pai, Hsiu-Hsuan Wang, Kai-Wei Chang, Alexander H. Liu, Hung-yi Lee<br/>
    *Preprint*<br/>
    [ [pdf](https://arxiv.org/abs/2402.13071) | [Github](https://github.com/voidful/Codec-SUPERB) | [Leaderboard](https://codecsuperb.com/) | [Huggingface](https://huggingface.co/Codec-SUPERB)]

- **Towards audio language modeling - an overview**<br/>
    <u>Haibin Wu</u>, Xuanjun Chen, Yi-Cheng Lin, Kai-wei Chang, Ho-Lam Chung, Alexander H. Liu, Hung-yi Lee<br/>
    *Preprint*<br/>
    [ [pdf](https://arxiv.org/abs/2402.13236)]

- **SpeechGen: Unlocking the Generative Power of Speech Language Models with Prompts**<br/>
    <u>Haibin Wu</u>, Kai-Wei Chang, Yuan-Kuei Wu, Hung-yi Lee<br/>
    *Preprint*<br/>
    [ [pdf](https://arxiv.org/abs/2306.02207)]

- **The defender's perspective on automatic speaker verification: An overview**<br/>
    <u>Haibin Wu</u>, Jiawen Kang, Lingwei Meng, Helen Meng, Hung-yi Lee<br/>
    *IJCAI DADA workshop 2023*<br/>
    [ [pdf](https://arxiv.org/abs/2305.12804)]

- **Rethinking complex-valued deep neural networks for monaural speech enhancement**<br/>
    <u>Haibin Wu</u>, Ke Tan, Buye Xu, Anurag Kumar, Daniel Wong<br/>
    *Interspeech 2023*<br/>
    [ [pdf](https://arxiv.org/abs/2301.04320)]

- **MFA-Conformer: Multi-scale Feature Aggregation Conformer for Automatic Speaker Verification**<br/>
    Yang Zhang, Zhiqiang Lv, <u>Haibin Wu</u>, etc.<br/>
    *Interspeech 2022*<br/>
    [ [pdf](https://arxiv.org/abs/2203.15249)]

- **Tackling Spoofing-Aware Speaker Verification with Multi-Model Fusion**<br/>
    <u>Haibin Wu</u>,  Jiawen Kang, Lingwei Meng, etc.<br/>
    *Odyssey 2022*<br/>
    [ [pdf](https://arxiv.org/abs/2206.09131)]

- **Partially Fake Audio Detection by Self-Attention-Based Fake Span Discovery**<br/>
    <u>Haibin Wu</u>, Heng-Cheng Kuo, Naijun Zheng, Kuo-Hsuan Hung, Hung-Yi Lee, Yu Tsao, Hsin-Min Wang, Helen Meng<br/>
    *ICASSP 2022*<br/>
    [ [pdf](https://arxiv.org/abs/2202.06684) | [video](https://www.youtube.com/watch?v=owPPvwN_Rfc)]

- **Adversarial Sample Detection for Speaker Verification by Neural Vocoders**<br/>
    <u>Haibin Wu</u>, Po-chun Hsu, Ji Gao, Shanshan Zhang, Shen Huang, Jian Kang, Zhiyong Wu, Helen Meng, Hung-yi Lee<br/>
    *ICASSP 2022*<br/>
    [ [pdf](https://arxiv.org/abs/2107.00309) | [Github](https://github.com/HaibinWu666/spot-adv-by-vocoder) | [video](https://youtu.be/7jD6iCzSgCM)]

- **Characterizing the adversarial vulnerability of speech self-supervised learning**<br/>
    <u>Haibin Wu</u>, Bo Zheng, Xu Li, Xixin Wu, Hung-yi Lee, Helen Meng<br/>
    *ICASSP 2022*<br/>
    [ [pdf](https://arxiv.org/abs/2111.04330) | [video](https://youtu.be/H_YVgo7y048)]

- **Voting for the right answer: Adversarial defense for speaker verification**<br/>
    <u>Haibin Wu</u>, Yang Zhang, Zhiyong Wu, Dong Wang, Hung-yi Lee<br/>
    *Interspeech 2021*<br/>
    [ [pdf](https://arxiv.org/abs/2106.07868) | [Github](https://github.com/thuhcsi/adsv_voting)]

- **Adversarial defense for automatic speaker verification by cascaded self-supervised learning models**<br/>
    <u>Haibin Wu</u>, Xu Li, Andy T. Liu, Zhiyong Wu, Helen Meng, Hung-yi Lee<br/>
    *ICASSP 2021*<br/>
    [ [pdf](https://arxiv.org/pdf/2102.07047.pdf) ]
    
- **Defense for Black-box Attacks on Anti-spoofing Models by Self-Supervised Learning**<br/>
    <u>Haibin Wu</u>, AT Liu, H Lee<br/>
    *Interspeech 2020*<br/>
    [ [pdf](https://arxiv.org/pdf/2006.03214.pdf) | [video](https://www.youtube.com/watch?v=k81atCYWpzg&t=666s) ]
    
- **Adversarial attacks on spoofing countermeasures of automatic speaker verification**<br/>
    S Liu, <u>H Wu</u>, H Lee, H Meng<br/>
    *ASRU 2019*<br/>
    [ [pdf](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9003763) ]

- **Multiview Learning for Subsurface Defect Detection in Composite Products: A Challenge on Thermographic Data Analysis**<br/>
    <u>Haibin Wu</u>, K Zheng, S Sfarra, Y Liu, Y Yao<br/>
    *IEEE Transactions on Industrial Informatics*<br/>
    [ [pdf](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8949715) ]

For the complete list, please visit [google scholar](https://scholar.google.com.tw/citations?user=-bB-WHEAAAAJ&hl=zh-TW).

# Research Experience
- <p style="display: flex; flex-direction: row; justify-content: space-between; margin: 0 0 0.5em;"><span style="flex: 0 0 auto">Research scientist intern at Microsoft</span> <span style="flex:  0 0 auto"><i>Feb 2024 - Present</i></span></p>

- <p style="display: flex; flex-direction: row; justify-content: space-between; margin: 0 0 0.5em;"><span style="flex: 0 0 auto">Research scientist intern at Meta</span> <span style="flex:  0 0 auto"><i>May 2023 - Sep 2023</i></span></p>

- <p style="display: flex; flex-direction: row; justify-content: space-between; margin: 0 0 0.5em;"><span style="flex: 0 0 auto">Applied scientist intern at Amazon</span> <span style="flex:  0 0 auto"><i>Sep 2022 - Dec 2022</i></span></p>

- <p style="display: flex; flex-direction: row; justify-content: space-between; margin: 0 0 0.5em;"><span style="flex: 0 0 auto">Research scientist intern at Meta</span> <span style="flex:  0 0 auto"><i>May 2022 - Aug 2022</i></span></p>

- <p style="display: flex; flex-direction: row; justify-content: space-between; margin: 0 0 0.5em;"><span style="flex: 0 0 auto">Visiting Student at the Chinese University of Hong Kong</span> <span style="flex:  0 0 auto"><i>May 2021 - April 2022</i></span></p>

- <p style="display: flex; flex-direction: row; justify-content: space-between; margin: 0 0 0.5em;"><span style="flex: 0 0 auto">Visiting Student at SIGS of Tsinghua University</span> <span style="flex:  0 0 auto"><i>Aug. 2020 - May 2021</i></span></p>

- <p style="display: flex; flex-direction: row; justify-content: space-between; margin: 0 0 0.5em;"><span style="flex: 0 0 auto">Intern at Tencent</span> <span style="flex:  0 0 auto"><i>Jan. 2021 - May 2021</i></span></p>


<!-- - <p style="display: flex; flex-direction: row; justify-content: space-between; margin: 0 0 0.5em;"><span style="flex: 0 0 auto">Research Assistant at National Tsinghua University</span> <span style="flex:  0 0 auto"><i>Sep. 2018 - Mar. 2019</i></span></p> -->

# Challenge
- <p style="display: flex; flex-direction: row; justify-content: space-between; margin: 0 0 0.5em;"><span style="flex: 0 0 auto"><a href="http://addchallenge.cn/add2022">2022 ICASSP ADD challenge track 2</a> </span> <span style="flex:  0 0 auto"><i>2/33</i></span></p>
- <p style="display: flex; flex-direction: row; justify-content: space-between; margin: 0 0 0.5em;"><span style="flex: 0 0 auto"><a href="https://www.alibabacloud.com/m2met-alimeeting">2022 ICASSP M2MeT challenge track 1</a> </span> <span style="flex:  0 0 auto"><i>2/14</i></span></p>
<!-- - <p style="display: flex; flex-direction: row; justify-content: space-between; margin: 0 0 0.5em;"><span style="flex: 0 0 auto">[2022 Interspeech SASV challenge](https://sasv-challenge.github.io/)</span> <span style="flex:  0 0 auto"><i>8/23</i></span></p> -->

# Honers
- <p style="display: flex; flex-direction: row; justify-content: space-between; margin: 0 0 0.5em;"><span style="flex: 0 0 auto">Interspeech travel grant</span> <span style="flex:  0 0 auto"><i>Interspeech 2022</i></span></p>
- <p style="display: flex; flex-direction: row; justify-content: space-between; margin: 0 0 0.5em;"><span style="flex: 0 0 auto">Appier Scholarship</span> <span style="flex:  0 0 auto"><i>Appier 2022</i></span></p>
- <p style="display: flex; flex-direction: row; justify-content: space-between; margin: 0 0 0.5em;"><span style="flex: 0 0 auto">Google PHD Fellowship</span> <span style="flex:  0 0 auto"><i>Google 2021</i></span></p>
- <p style="display: flex; flex-direction: row; justify-content: space-between; margin: 0 0 0.5em;"><span style="flex: 0 0 auto">Advanced Speech Technologies Scholarship</span> <span style="flex:  0 0 auto"><i>NTU EECS 2020</i></span></p>
- <p style="display: flex; flex-direction: row; justify-content: space-between; margin: 0 0 0.5em;"><span style="flex: 0 0 auto">Academic Achievement Award</span> <span style="flex:  0 0 auto"><i>NCTU EECS 2019</i></span></p>
- <p style="display: flex; flex-direction: row; justify-content: space-between; margin: 0 0 0.5em;"><span style="flex: 0 0 auto">Academic Achievement Award</span> <span style="flex:  0 0 auto"><i>NCTU EECS 2018</i></span></p>
- <p style="display: flex; flex-direction: row; justify-content: space-between; margin: 0 0 0.5em;"><span style="flex: 0 0 auto">National Scholarship</span> <span style="flex:  0 0 auto"><i>Chinese Ministry of Education 2014</i></span></p>

# Teaching

- <p style="display: flex; flex-direction: row; justify-content: space-between; margin: 0 0 0.5em;"><span style="flex: 0 0 auto">TA of <a href="https://www.youtube.com/watch?v=Ye018rCVvOo&list=PLJV_el3uVTsMhtt7_Y6sgTHGHp1Vb2P2J/">Machine Learning</a></span> <span style="flex:  0 0 auto"><i>NTU EECS, Spring 2021</i></span></p>


<!-- # Projects

- Open Sourced End-to-end Speech Recognition System [ [code](https://github.com/Alexander-H-Liu/End-to-end-ASR-Pytorch) ![GitHub stars](https://img.shields.io/github/stars/Alexander-H-Liu/End-to-end-ASR-Pytorch?style=social&label=Star&maxAge=2592000) ]
- Mandarin Spoken QA System [ *[demo](http://deeplearning.website:8080/?fbclid=IwAR1G6mdk34Q9vA29KhKyn7AFNblR2iV3c2N21H7AbKXS9EN9VuFgO8vIrBE)* ] -->
